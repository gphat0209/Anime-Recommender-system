{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZJlJP-y05dje",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJlJP-y05dje",
        "outputId": "4ef77dfb-b92e-41c4-f3b7-a261abb6c93d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5796OQgx57H9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5796OQgx57H9",
        "outputId": "644237a5-03e7-4014-85e7-e3a7bbd4b764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Fine-tuning\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eJPujE0J6LED",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJPujE0J6LED",
        "outputId": "c616f354-a163-4b84-c9ff-def5eaaaf63c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m'anime model'\u001b[0m/   prepare_data.py    \u001b[01;34mroberta_cv_models\u001b[0m/   training_colab.ipynb\n",
            " \u001b[01;34manime_model\u001b[0m/    \u001b[01;34m__pycache__\u001b[0m/       \u001b[01;34mroberta_model\u001b[0m/       \u001b[01;34mtraining_data\u001b[0m/\n",
            " finetune.py     requirements.txt   \u001b[01;34mt5_anime_model\u001b[0m/      utils.py\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da0de755",
      "metadata": {
        "id": "da0de755"
      },
      "source": [
        "## Download essential packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c517648e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c517648e",
        "outputId": "c34d0183-0b30-4f7a-8f1c-39ff114b91ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.56.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (0.35.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 6)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 6)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 6)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 6)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 2)) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (1.20.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4920af7d",
      "metadata": {
        "id": "4920af7d"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4809abf",
      "metadata": {
        "id": "e4809abf"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import argparse\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8517f4e9",
      "metadata": {
        "id": "8517f4e9"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q_XmRo-rF2z7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_XmRo-rF2z7",
        "outputId": "7491193a-1736-40d3-98d9-0265e2151f8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Device count: 1\n",
            "Current device: 0\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device count:\", torch.cuda.device_count())\n",
        "print(\"Current device:\", torch.cuda.current_device())\n",
        "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81630eea",
      "metadata": {
        "id": "81630eea"
      },
      "outputs": [],
      "source": [
        "from utils import get_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xqZbfSOS7HqA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqZbfSOS7HqA",
        "outputId": "73b4310d-c2c7-4d6e-99bf-c143d9964c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss: 1.3903 | Accuracy: 0.2663\n",
            "‚úÖ Model saved at roberta_model/epoch_1\n",
            "Epoch 2 | Loss: 1.3780 | Accuracy: 0.2868\n",
            "‚úÖ Model saved at roberta_model/epoch_2\n",
            "Epoch 3 | Loss: 1.3063 | Accuracy: 0.3611\n",
            "‚úÖ Model saved at roberta_model/epoch_3\n",
            "Epoch 4 | Loss: 1.1840 | Accuracy: 0.4551\n",
            "‚úÖ Model saved at roberta_model/epoch_4\n",
            "Epoch 5 | Loss: 1.0185 | Accuracy: 0.5408\n",
            "‚úÖ Model saved at roberta_model/epoch_5\n",
            "Epoch 6 | Loss: 0.7578 | Accuracy: 0.6642\n",
            "‚úÖ Model saved at roberta_model/epoch_6\n",
            "Epoch 7 | Loss: 0.5615 | Accuracy: 0.7696\n",
            "‚úÖ Model saved at roberta_model/epoch_7\n",
            "Epoch 8 | Loss: 0.4117 | Accuracy: 0.8440\n",
            "‚úÖ Model saved at roberta_model/epoch_8\n",
            "Epoch 9 | Loss: 0.3080 | Accuracy: 0.8848\n",
            "‚úÖ Model saved at roberta_model/epoch_9\n",
            "Epoch 10 | Loss: 0.1522 | Accuracy: 0.9502\n",
            "‚úÖ Model saved at roberta_model/epoch_10\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "model_name = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=4\n",
        ").to(device)\n",
        "\n",
        "train_loader = get_loader(\"train\", \"training_data/train.json\", tokenizer, batch_size=4)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "save_dir = \"roberta_model\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        acc = (preds == labels).float().mean().item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "        total_batches += 1\n",
        "\n",
        "    avg_loss = epoch_loss / total_batches\n",
        "    avg_acc = epoch_acc / total_batches\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "    # üî• l∆∞u model + tokenizer cu·ªëi m·ªói epoch\n",
        "    epoch_dir = os.path.join(save_dir, f\"epoch_{epoch+1}\")\n",
        "    os.makedirs(epoch_dir, exist_ok=True)\n",
        "    model.save_pretrained(epoch_dir)\n",
        "    tokenizer.save_pretrained(epoch_dir)\n",
        "    print(f\"‚úÖ Model saved at {epoch_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7CCqPTL_JyY9",
      "metadata": {
        "id": "7CCqPTL_JyY9"
      },
      "source": [
        "Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bWTCN7mJ0px",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bWTCN7mJ0px",
        "outputId": "e7f0b349-5fdf-4f92-86c6-aa1df211b1db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================ Fold 1 / 5 ================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 204/204 [01:21<00:00,  2.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 1: Loss=1.3846, Acc=0.2690\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 204/204 [01:21<00:00,  2.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 2: Loss=1.3600, Acc=0.2812\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 204/204 [01:21<00:00,  2.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 3: Loss=1.3109, Acc=0.3413\n",
            "‚úÖ Fold 1 | Val Loss: 1.3000 | Val Acc: 0.3716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================ Fold 2 / 5 ================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:21<00:00,  2.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 1: Loss=1.3866, Acc=0.2774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:21<00:00,  2.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 2: Loss=1.3772, Acc=0.2890\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:21<00:00,  2.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 3: Loss=1.3533, Acc=0.3238\n",
            "‚úÖ Fold 2 | Val Loss: 1.3919 | Val Acc: 0.2892\n",
            "\n",
            "================ Fold 3 / 5 ================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 3 Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:21<00:00,  2.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 1: Loss=1.3893, Acc=0.2543\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:20<00:00,  2.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 2: Loss=1.3869, Acc=0.2616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:20<00:00,  2.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 3: Loss=1.3866, Acc=0.2646\n",
            "‚úÖ Fold 3 | Val Loss: 1.3756 | Val Acc: 0.2990\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================ Fold 4 / 5 ================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 4 Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:21<00:00,  2.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 1: Loss=1.3873, Acc=0.2671\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 4 Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:21<00:00,  2.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 2: Loss=1.3787, Acc=0.2683\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 4 Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:21<00:00,  2.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 3: Loss=1.3558, Acc=0.3006\n",
            "‚úÖ Fold 4 | Val Loss: 1.3426 | Val Acc: 0.3480\n",
            "\n",
            "================ Fold 5 / 5 ================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 5 Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:21<00:00,  2.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 1: Loss=1.3852, Acc=0.2787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 5 Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:21<00:00,  2.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 2: Loss=1.3555, Acc=0.2945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 5 Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:21<00:00,  2.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 3: Loss=1.2881, Acc=0.3780\n",
            "‚úÖ Fold 5 | Val Loss: 1.3598 | Val Acc: 0.3578\n",
            "\n",
            "================ Summary ================\n",
            "Average Val Loss: 1.3540\n",
            "Average Val Accuracy: 0.3332\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class My_Dataset(Dataset):\n",
        "    def __init__(self, json_path, tokenizer, max_length=256):\n",
        "        \"\"\"\n",
        "        json_path: ƒë∆∞·ªùng d·∫´n file JSON ch·ª©a d·ªØ li·ªáu (c√≥ 'input' v√† 'label')\n",
        "        tokenizer: tokenizer t·ª´ m√¥ h√¨nh HuggingFace (vd: RobertaTokenizer)\n",
        "        max_length: ƒë·ªô d√†i t·ªëi ƒëa khi tokenize\n",
        "        \"\"\"\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.data = json.load(f)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.data[idx]\n",
        "        text = record[\"text\"]\n",
        "        label = record[\"label\"]  # ƒë√£ l√† int (0, 1, 2, 3, ...)\n",
        "\n",
        "        # Tokenize vƒÉn b·∫£n\n",
        "        encodings = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encodings[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model_name = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# dataset & dataloader t·ª´ h√†m b·∫°n c√≥ s·∫µn\n",
        "full_dataset = My_Dataset(\"training_data/full.json\", tokenizer)\n",
        "\n",
        "# 5-Fold Cross Validation\n",
        "k_folds = 5\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# ========================\n",
        "# 2Ô∏è‚É£ Train Loop for K-Folds\n",
        "# ========================\n",
        "save_dir = \"roberta_cv_models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(full_dataset)):\n",
        "    print(f\"\\n================ Fold {fold+1} / {k_folds} ================\")\n",
        "\n",
        "    # Subset theo fold\n",
        "    train_subset = Subset(full_dataset, train_idx)\n",
        "    val_subset = Subset(full_dataset, val_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=8, shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=8, shuffle=False)\n",
        "\n",
        "    # Model m·ªõi cho m·ªói fold (kh√¥ng d√πng l·∫°i weight fine-tuned)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=4\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "    # Train trong 3 epoch ƒë·ªÉ demo\n",
        "    for epoch in range(3):\n",
        "        model.train()\n",
        "        total_loss, total_acc = 0, 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}\"):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            acc = (preds == labels).float().mean().item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc\n",
        "\n",
        "        print(f\"Train Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Acc={total_acc/len(train_loader):.4f}\")\n",
        "\n",
        "    # =====================\n",
        "    # Validation cho fold\n",
        "    # =====================\n",
        "    model.eval()\n",
        "    val_correct, val_total, val_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"‚úÖ Fold {fold+1} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    fold_results.append((val_loss, val_acc))\n",
        "\n",
        "    # L∆∞u model m·ªói fold\n",
        "    fold_dir = os.path.join(save_dir, f\"fold_{fold+1}\")\n",
        "    os.makedirs(fold_dir, exist_ok=True)\n",
        "    model.save_pretrained(fold_dir)\n",
        "    tokenizer.save_pretrained(fold_dir)\n",
        "\n",
        "# ============================\n",
        "# 3Ô∏è‚É£ T·ªïng h·ª£p k·∫øt qu·∫£ K-Fold\n",
        "# ============================\n",
        "avg_loss = np.mean([r[0] for r in fold_results])\n",
        "avg_acc = np.mean([r[1] for r in fold_results])\n",
        "print(\"\\n================ Summary ================\")\n",
        "print(f\"Average Val Loss: {avg_loss:.4f}\")\n",
        "print(f\"Average Val Accuracy: {avg_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ubbxL5UYaez",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ubbxL5UYaez",
        "outputId": "1609605d-fdfd-42aa-903b-79890000e0f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Train acc: 0.5913767760901519\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import json\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# 1. Load data\n",
        "data = json.load(open(\"training_data/full.json\"))\n",
        "texts = [d[\"text\"] for d in data]\n",
        "labels = [d[\"label\"] for d in data]\n",
        "\n",
        "# 2. Embedding model\n",
        "GOOGLE_API_KEY = \"AIzaSyCdnd_6OOWJ3ZifpvL-2X6KiKzlMseJrfY\"\n",
        "# embeddings = GoogleGenerativeAIEmbeddings(\n",
        "#     model=\"models/text-embedding-004\",\n",
        "#     google_api_key=GOOGLE_API_KEY\n",
        "# )\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-exp-03-07\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# 3. Encode to√†n b·ªô\n",
        "embs = embeddings.embed_documents(texts)\n",
        "\n",
        "# 4. Train classifier\n",
        "clf = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "clf.fit(embs, labels)\n",
        "\n",
        "# 5. Evaluate tr√™n c√πng t·∫≠p (ch·ªâ ƒë·ªÉ ki·ªÉm tra fitting)\n",
        "preds = clf.predict(embs)\n",
        "print(\"‚úÖ Train acc:\", accuracy_score(labels, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fm3iSI7BVpjE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "fm3iSI7BVpjE",
        "outputId": "c1eb4a54-eaec-4162-c0b1-48e01b91b18e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Train acc: 0.5913767760901519\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-231261217.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Train acc:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LRmodels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LRmodels/mlp_anime_classifier.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import json\n",
        "import joblib\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "data = json.load(open(\"training_data/full.json\"))\n",
        "texts = [d[\"text\"] for d in data]\n",
        "labels = [d[\"label\"] for d in data]\n",
        "\n",
        "GOOGLE_API_KEY = \"\"\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-exp-03-07\",\n",
        " #   model=\"models/text-embedding-004\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "embs = embeddings.embed_documents(texts)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "clf.fit(embs, labels)\n",
        "\n",
        "preds = clf.predict(embs)\n",
        "print(\"‚úÖ Train acc:\", accuracy_score(labels, preds))\n",
        "\n",
        "os.makedirs(\"LRmodels\", exist_ok=True)\n",
        "\n",
        "joblib.dump(clf, \"LRmodels/mlp_anime_classifier.pkl\")\n",
        "print(\"‚úÖ Saved model to models/mlp_anime_classifier.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kZzFFMKWbpf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZzFFMKWbpf5",
        "outputId": "eeacd8de-9472-4523-b182-1ecf8101118b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Saved model to models/mlp_anime_classifier.pkl\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "os.makedirs(\"LRmodels\", exist_ok=True)\n",
        "\n",
        "joblib.dump(clf, \"LRmodels/lr_anime_classifier.pkl\")\n",
        "print(\"‚úÖ Saved model to LRmodels/lr_anime_classifier.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mhMM_XkxxpHT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "mhMM_XkxxpHT",
        "outputId": "b0e6c645-1b23-4d77-a2a7-74b52e5394df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:18<00:00,  1.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss: 1.3185 | Accuracy: 0.2514\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:17<00:00,  1.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Loss: 0.0992 | Accuracy: 0.2801\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:16<00:00,  1.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Loss: 0.0961 | Accuracy: 0.2808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4:  30%|‚ñà‚ñà‚ñà       | 27/90 [00:23<00:55,  1.13it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1575976108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtotal_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "class T5Dataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, max_length=512):\n",
        "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.data = json.load(f)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.data[idx]\n",
        "        input_text = record[\"input\"]\n",
        "        target_text = record[\"output\"]\n",
        "\n",
        "        input_enc = self.tokenizer(\n",
        "            input_text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        target_enc = self.tokenizer(\n",
        "            target_text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=16,  # output ng·∫Øn\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_enc[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": target_enc[\"input_ids\"].squeeze()\n",
        "        }\n",
        "\n",
        "def get_loader(path, tokenizer, batch_size=4, mode=\"train\"):\n",
        "    dataset = T5Dataset(path, tokenizer)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(mode==\"train\"),\n",
        "        num_workers=2\n",
        "    )\n",
        "    return loader\n",
        "\n",
        "\n",
        "# =============================\n",
        "# 2Ô∏è‚É£ Kh·ªüi t·∫°o model v√† optimizer\n",
        "# =============================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model_name = \"google/flan-t5-small\"  # ho·∫∑c \"t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "train_loader = get_loader(\"training_data/instruction_prompt/train.json\", tokenizer, batch_size=16)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
        "\n",
        "# =============================\n",
        "# 3Ô∏è‚É£ Hu·∫•n luy·ªán & L∆∞u model\n",
        "# =============================\n",
        "save_dir = \"t5_anime_model\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # ---- Forward ----\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "        # ---- Decode ƒë·ªÉ t√≠nh accuracy ----\n",
        "        preds = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=16\n",
        "        )\n",
        "\n",
        "        pred_texts = [tokenizer.decode(p, skip_special_tokens=True).strip().lower() for p in preds]\n",
        "        label_texts = [tokenizer.decode(l, skip_special_tokens=True).strip().lower() for l in labels]\n",
        "\n",
        "        for p, l in zip(pred_texts, label_texts):\n",
        "            if p == l:\n",
        "                total_correct += 1\n",
        "            total_samples += 1\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "    avg_acc = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "# ‚úÖ L∆∞u model + tokenizer sau khi hu·∫•n luy·ªán xong t·∫•t c·∫£ c√°c epoch\n",
        "final_dir = os.path.join(save_dir, \"final_model\")\n",
        "os.makedirs(final_dir, exist_ok=True)\n",
        "model.save_pretrained(final_dir)\n",
        "tokenizer.save_pretrained(final_dir)\n",
        "print(f\"‚úÖ Training complete! Model saved at {final_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rs_HiI-n4HYc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs_HiI-n4HYc",
        "outputId": "b853ee43-9f9f-486a-83d5-effcc0e14ece"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:20<00:00,  3.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Test Loss: 0.1015 | Test Accuracy: 0.2953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "def evaluate_t5(model, tokenizer, test_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # üëâ T√≠nh loss\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            total_loss += outputs.loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "            # üëâ Sinh d·ª± ƒëo√°n text\n",
        "            preds = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=16\n",
        "            )\n",
        "\n",
        "            pred_texts = [tokenizer.decode(p, skip_special_tokens=True).strip().lower() for p in preds]\n",
        "            label_texts = [tokenizer.decode(l, skip_special_tokens=True).strip().lower() for l in labels]\n",
        "\n",
        "            # üëâ T√≠nh accuracy\n",
        "            for p, l in zip(pred_texts, label_texts):\n",
        "                if p == l:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "    accuracy = correct / total if total > 0 else 0.0\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "# Load model & tokenizer ƒë√£ hu·∫•n luy·ªán\n",
        "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "save_dir = \"t5_anime_model/final_model\"  # n∆°i b·∫°n ƒë√£ l∆∞u model sau train\n",
        "model = T5ForConditionalGeneration.from_pretrained(save_dir).to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(save_dir)\n",
        "\n",
        "# T·∫°o dataloader cho t·∫≠p test\n",
        "\n",
        "def get_loader2(json_path, tokenizer, batch_size=8):\n",
        "    dataset = T5Dataset(json_path, tokenizer)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = get_loader2( \"training_data/instruction_prompt/test.json\", tokenizer, batch_size=8)\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = evaluate_t5(model, tokenizer, test_loader, device)\n",
        "print(f\"üìä Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zM3Mbd_k9G_H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM3Mbd_k9G_H",
        "outputId": "f9c40246-93e4-4426-cd22-af54b823ef22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Test Loss: 3.6284 | Test Accuracy: 0.2583\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import functional as F\n",
        "from utils import get_loader\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "from transformers import RobertaForSequenceClassification\n",
        "save_dir = \"./roberta_model/epoch_10\"\n",
        "model = RobertaForSequenceClassification.from_pretrained(save_dir)\n",
        "model.to(device)\n",
        "# üîé G·ªçi evaluate sau khi train\n",
        "test_loader = get_loader(\"test\", \"training_data/test2.json\", tokenizer, batch_size=4)\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader, device)\n",
        "print(f\"üìä Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W7b0MMRp-9wL",
      "metadata": {
        "id": "W7b0MMRp-9wL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
